<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Advanced Features · datadocs</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="&lt;h2&gt;&lt;a class=&quot;anchor&quot; aria-hidden=&quot;true&quot; id=&quot;group-based-features&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#group-based-features&quot; aria-hidden=&quot;true&quot; class=&quot;hash-link&quot;&gt;&lt;svg class=&quot;hash-link-icon&quot; aria-hidden=&quot;true&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Group-based features&lt;/h2&gt;
"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Advanced Features · datadocs"/><meta property="og:type" content="website"/><meta property="og:url" content="https://polakowo.github.io/datadocs/"/><meta property="og:description" content="&lt;h2&gt;&lt;a class=&quot;anchor&quot; aria-hidden=&quot;true&quot; id=&quot;group-based-features&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#group-based-features&quot; aria-hidden=&quot;true&quot; class=&quot;hash-link&quot;&gt;&lt;svg class=&quot;hash-link-icon&quot; aria-hidden=&quot;true&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Group-based features&lt;/h2&gt;
"/><meta property="og:image" content="https://polakowo.github.io/datadocs/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://polakowo.github.io/datadocs/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/datadocs/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/zenburn.min.css"/><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/datadocs/js/code-block-buttons.js"></script><script src="/datadocs/js/scrollSpy.js"></script><link rel="stylesheet" href="/datadocs/css/prism.css"/><link rel="stylesheet" href="/datadocs/css/main.css"/><script src="/datadocs/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/datadocs/"><img class="logo" src="/datadocs/img/favicon.ico" alt="datadocs"/><h2 class="headerTitleWithLogo">datadocs</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/datadocs/docs/machine-learning/linear-models" target="_self">Docs</a></li><li class=""><a href="https://github.com/polakowo/datadocs" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Features</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Machine Learning<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Methods</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/linear-models">Linear Models</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/tree-based-models">Tree-Based Models</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/ensemble-methods">Ensemble Methods</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Features</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/eda">Exploratory Data Analysis</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/feature-engineering">Feature Engineering</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/datadocs/docs/machine-learning/advanced-features">Advanced Features</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Optimization</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/metric-optimization">Metric Optimization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/validation-schemes">Validation Schemes</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/hyperopt">Hyperparameter Optimization</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Competitions</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/competitive-ml">Competitive Machine Learning</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/data-leakages">Data Leakages</a></li></ul></div></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Deep Learning<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">General</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/deep-learning">Deep Learning</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/dl-strategy">Deep Learning Strategy</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Fundamentals</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/backpropagation">Backpropagation</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/activation-functions">Activation Functions</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/initialization">Initialization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/optimization">Optimization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/regularization">Regularization</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Computer Vision</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/cnns">Convolutional Neural Networks</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/cnn-architectures">CNN Architectures</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/object-detection">Object Detection</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/face-recognition">Face Recognition</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/nst">Neural Style Transfer</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Natural Language Processing</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/rnns">Recurrect Neural Networks</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/word-embeddings">Word Embeddings</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/nmt">Neural Machine Translation</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/attention-mechanism">Attention Mechanism</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/speech-recognition">Speech Recognition</a></li></ul></div></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/polakowo/datadocs/edit/master/docs/machine-learning/advanced-features.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 class="postHeaderTitle">Advanced Features</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="group-based-features"></a><a href="#group-based-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Group-based features</h2>
<ul>
<li>Some datasets such as transactional data have multiple rows for an instance.</li>
<li>Treat data points as dependent on each other.</li>
<li>Group by a categorical feature and calculate various statistics such as sum and mean (e.g., mean encoding).</li>
<li>To group by a numeric feature apply binning. Binning can be applied on both categorical and numerical data to make the model more robust and prevent overfitting, however, it has a cost to the performance.</li>
<li>For aggregating categorical features either:
<ul>
<li>Select the label with the highest frequency</li>
<li>Make a pivot table</li>
<li>Apply <code>groupby</code> operation after one-hot-transforming the feature.</li>
</ul></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="distance-based-features"></a><a href="#distance-based-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Distance-based features</h2>
<ul>
<li>Perform a <code>groupby</code> operation on instance neighborhoods (not only geodesic).</li>
<li>Explicit group is not needed</li>
<li>More flexible but harder to implement:
<ul>
<li>(optional) Mean encode all variables to create a homogeneous feature space.</li>
<li>Calculate \(N\) nearest neighbors with some distance metric (e.g., Bray-Curtis).
<center><img width=200 src="/datadocs/assets/knn3.png"/></center>
<center><a href="https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/" style="color: lightgrey">Credit</a></center></li>
<li>Calculate various statistics based on the nearest \(K\) neighbors.</li>
<li>Examples:
<ul>
<li>Mean target of nearest 5, 10, 15, 500, 2000 neighbors</li>
<li>Mean distance to 10 closest neighbors (with target 0/1)</li>
</ul></li>
</ul></li>
<li>For pairs of text features calculate:
<ul>
<li>The number of matching words</li>
<li>Cosine distance between their TF-IDF vectors</li>
<li>Distance between their average word2vec vectors</li>
<li>Levenshtein distance</li>
</ul></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="bumper-features"></a><a href="#bumper-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Bumper features</h2>
<ul>
<li>For a multi-classification problem, create one-versus-all tasks and use them as features.</li>
<li><em>Is it true that the target class number is greater than 1, 2, etc.?</em></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="feature-interactions"></a><a href="#feature-interactions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Feature interactions</h2>
<ul>
<li>Construct combinations of features in order to incorporate the knowledge into a model.</li>
<li>For categorical features:
<ul>
<li>Concatenate them by using a delimiter.</li>
<li>Or vectorize into a real-valued representation and then apply the operation.</li>
</ul></li>
<li>There are \(N^2*M\) possible interactions for \(N\) features and \(M\) operations:
<ul>
<li>Reduce them by feature selection (e.g., with RF) or matrix factorization.</li>
</ul></li>
<li>Such approach can be generalized for higher orders.</li>
<li>Due to the fact that number of features grows rapidly with order, they are often constructed semi-manually.</li>
<li>Extract interactions via decision trees by using indices of the leafs (<code>model.apply</code>).
<ul>
<li>Two factors that are split in succession might indicate an interaction.</li>
<li><a href="https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/">Facebook Research's paper about extracting categorical features from trees</a></li>
<li><a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html">Example: Feature transformations with ensembles of trees (sklearn)</a></li>
</ul></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="matrix-factorization"></a><a href="#matrix-factorization" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Matrix factorization</h2>
<ul>
<li>Matrix factorization is a generic approach for dimensionality reduction and feature extraction.</li>
<li>Can provide additional diversity which is good for ensembles.</li>
<li>It is a lossy transformation which efficiency depends on the task and the number of latent factors.</li>
<li>The same transformation tricks as for linear models should be used.</li>
<li>The same parameters should be used throughout the dataset as it's a trainable transformation.</li>
<li><a href="http://scikit-learn.org/stable/modules/decomposition.html">Overview of Matrix Decomposition methods (sklearn)</a></li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="pca"></a><a href="#pca" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>PCA:</h4>
<ul>
<li>PCA (Principal Component Analysis) performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. It does so by calculating the eigenvectors from the covariance matrix.</li>
<li>Can drop the least important feature while still retaining the most valuable parts.</li>
<li>In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the <code>explained_variance_ratio</code>)</li>
<li>Each of the new features or components created after PCA are all independent of one another.</li>
<li>Mainly applied to dense data.</li>
<li>Limitations:
<ul>
<li>PCA is a linear algorithm (not able to interpret complex polynomial relationships between features).</li>
<li>Visualization and interpretation difficulties.</li>
<li>Strongly focused on variance which may not correlate with predictive power.</li>
</ul></li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="svd"></a><a href="#svd" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SVD:</h4>
<ul>
<li>SVD (Singular Value Decomposition) is a factorization of a matrix.</li>
<li>Same advantages and limitations as for PCA but faster.</li>
<li><code>TruncatedSVD</code> can be applied to sparse matrices.</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="nmf"></a><a href="#nmf" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>NMF:</h4>
<ul>
<li>NMF (Nonnegative Matrix Factorization) is a state of the art feature extraction algorithm.</li>
<li>Automatically extracts sparse and meaningful features from a set of nonnegative data vectors (counts-like data).</li>
<li>NMF decomposes a data matrix \(V\) into the product of two lower rank matrices \(W\) and \(H\) so that \(V\approx{W*H}\).</li>
</ul>
<p><center><img width=550 src="/datadocs/assets/holdout.png"/></center>
<center><a href="http://alexhwilliams.info/itsneuronalblog/2018/02/26/crossval/" style="color: lightgrey">Credit</a></center></p>
<ul>
<li>Primarily used for recommender systems and text mining.</li>
<li>Provides an additive basis to represent the data.</li>
<li>Results are easier to interpret than SVD.</li>
<li>Transforms data in a way best-suited for tree-based models.</li>
<li>NMF typically benefits from normalization.</li>
<li>Limitations:
<ul>
<li>As opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard.
<ul>
<li>Fortunately there are heuristic approximations.</li>
</ul></li>
<li>There is no guarantee to be a single unique decomposition.</li>
<li>It’s hard to know how to choose the factorisation rank \(r\).
<ul>
<li>Some approaches include trial and error.</li>
</ul></li>
</ul></li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="t-sne"></a><a href="#t-sne" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>t-SNE:</h4>
<ul>
<li>t-SNE (t-Distributed Stochastic Neighbor Embedding) is a tool to visualize high-dimensional data.</li>
<li>Unlike PCA, is not a linear projection but allows to capture a non-linear structure.</li>
</ul>
<p><center><img width=250 src="/datadocs/assets/PCASwiss.png"/></center>
<center><a href="https://www.biostars.org/p/295174/" style="color: lightgrey">Credit</a></center></p>
<ul>
<li>Projects points from high to low-dimensional space by preserving the relative distance between them.
<ul>
<li>Neighbor embedding is a search for a new and less-dimensional data representation that preserves neighborship of examples.</li>
</ul></li>
<li>Optimizes the embeddings directly using gradient descent.</li>
<li>The principal components can be used as features.</li>
<li>Mainly a data exploration and visualization technique.</li>
<li>Apply t-SNE to concatenation of train and test and split projection back.</li>
<li>Limitations:
<ul>
<li>Computationally expensive and can take several hours on million-sample datasets :
<ul>
<li>It is common to do dimensionality reduction before projection.</li>
<li>Use stand-alone implementation <code>tsne</code> for faster speed.</li>
</ul></li>
<li>Sometimes it works well for visualization but not for dimensionality reduction.</li>
<li>Results strongly depend on hyperparameters (perplexity):
<ul>
<li>Good practice is to use several projections with different perplexities (5-100).</li>
<li><a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py">Example: tSNE with different perplexities (sklearn)</a></li>
</ul></li>
<li>Due to stochastic nature, tSNE may provide different projects for the same configuration.</li>
<li>PCA it is a mathematical technique, but t-SNE is a probabilistic one.</li>
<li>There is the risk of getting stuck in local minima.</li>
</ul></li>
<li><a href="https://github.com/DmitryUlyanov/Multicore-TSNE">Multicore t-SNE implementation</a></li>
<li><a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html">Comparison of Manifold Learning methods (sklearn)</a></li>
<li><a href="https://distill.pub/2016/misread-tsne/">How to Use t-SNE Effectively (distill.pub blog)</a></li>
</ul>
</span></div></article></div><div class="docLastUpdate"><em>Last updated on 6/19/2019 by Oleg Polakow</em></div><div class="docs-prevnext"><a class="docs-prev button" href="/datadocs/docs/machine-learning/feature-engineering"><span class="arrow-prev">← </span><span>Feature Engineering</span></a><a class="docs-next button" href="/datadocs/docs/machine-learning/metric-optimization"><span>Metric Optimization</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#group-based-features">Group-based features</a></li><li><a href="#distance-based-features">Distance-based features</a></li><li><a href="#bumper-features">Bumper features</a></li><li><a href="#feature-interactions">Feature interactions</a></li><li><a href="#matrix-factorization">Matrix factorization</a></li></ul></nav></div><footer class="nav-footer" id="footer"><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/datadocs/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2019 Oleg Polakow</section></footer></div></body></html>